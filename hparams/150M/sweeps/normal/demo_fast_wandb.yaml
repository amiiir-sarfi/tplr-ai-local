# Wandb sweep for 150M model with 3.7B budget using normal strategy with Demo optimizer

name: demo_fast_sweep_150M_32M
project:  Demo150M_Buget32M_fast
method: grid
program: neurons/torch_baseline.py
command:
  - /usr/bin/env
  - torchrun
  - --nproc_per_node=8
  - ${program} 
  - ${args_no_boolean_flags}

parameters:
  # Fixed parameters
  hparams_file:
    value: $HOME/tplr-ai-local/hparams/150M/32M_budget_fast.json
  use_compile:
    value: True
  run_name:
    value: demo
  sequence_length:
    value: 2048
  strategy:
    value: normal
  outer_optimizer:
    value: demo

  # Batch size
  micro_batch_size:
    value: 8
  batch_size:
    value: 8 # With workers=8, bs=8, seqlen=2048 => effective_bs=2**17

  # Important parameters
  compression_decay:
    value: 0.999
  weight_decay:
    value: 0.0
  warmup_steps:
    value: 0.05
  outer_learning_rate:
    values:
      - 4e-3
      - 4e-4
      - 6e-4
      - 1e-3