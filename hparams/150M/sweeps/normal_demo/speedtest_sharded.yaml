# Wandb sweep to test speed of shardedData with and without torch compile on BS=8.

name: Demo150M_speedtest
project:  Demo150M_speedtest
method: grid
program: neurons/torch_baseline_sharded.py # Using sharded data
command:
  - /usr/bin/env
  - torchrun
  - --nproc_per_node=8
  - ${program} 
  - ${args_no_boolean_flags}

parameters:
  # Fixed parameters
  hparams_file:
    value: $HOME/tplr-ai-local/hparams/150M/150M_model_hparams.json
  run_name:
    value: shardedData
  sequence_length:
    value: 2048
  strategy:
    value: normal
  outer_optimizer:
    value: demo
  debug:
    value: True 
  # Batch size
  token_budget:
    value: 15728640 # effective_bs=(32 * 4 * 2048 = 2**17) * iterations=120
  data_in_gpu:
    value: True 
  micro_batch_size:
    value: -1
  batch_size:
    value: 8 # With workers=8, bs=8, seqlen=2048 => effective_bs=2**17

  # Important parameters
  compression_decay:
    value: 0.999
  weight_decay:
    value: 0.0
  warmup_steps:
    value: 0.05
  outer_learning_rate:
    value: 4e-4
  use_compile:
    values: 
    - True
    - False